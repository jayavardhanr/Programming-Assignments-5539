{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Neural Network Basics and Word2Vec\n",
    "AU 16 CSE 5539-0010 \"Social Media and Text Analysis\" Homework #3  \n",
    "Wei Xu, The Ohio State University   \n",
    "\n",
    "In this assignment, we will walk you through the process of implementing: \n",
    "\n",
    "- A softmax function and a sigmoid function\n",
    "- A simple neural network with back propagation\n",
    "- Word2vec models (Skip-gram, CBOW) with negative sampling\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD). The purpose of this assignment is to familiarize you with basic knowledge about neural networks and help you gain proficiency in writing efficient, vectorized code.\n",
    "\n",
    "You may find this **[documentation](http://cs224d.stanford.edu/assignment1/assignment1_soln)** very helpful, as it will walk you through all the math needed for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "IMPORTANG: In this assignment, except Numpy and Matplotlib, no other external Python packages are allowed. Scipy is used in gradient checking, though, it is not allowed elsewhere. Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the Ohio State University's Code of Student Conduct, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(Peddamail Jayavardhan Reddy)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Basics [Bonus 5 Points]\n",
    "\n",
    "In this part, you're going to implement\n",
    "\n",
    "* A softmax function\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Softmax\n",
    "\n",
    "The softmax function is defined as $softmax(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j}{e^{x_j}}}$. \n",
    "\n",
    "\n",
    "And conveniently, softmax is invarint to constant offsets in the input, that is, for any input vector $\\mathbf{x}$ and any constant $c$, $softmax(\\mathbf{x})=softmax(\\mathbf{x}+c)$, where $\\mathbf{x}+c$ means adding the constant $c$ to every dimension of $\\mathbf{x}$. \n",
    "\n",
    "**Show your efficient implementation of the softmax function in Python below. The computional efficienicy is crucial because this function will be used frequently in later code. ** \n",
    "\n",
    "You will find numpy functions np.exp, np.sum, np.reshape, np.max, and [numpy broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) useful. Numpy is a Python package for scientific programming. You can find a nice **Numpy tutoral** [here](http://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    X_=np.atleast_2d(x)\n",
    "    X_ -= np.max(X_,axis=1).reshape(-1,1)    \n",
    "    X_ = np.exp(X_)\n",
    "    X_ /= np.sum(X_,axis=1).reshape(-1,1)\n",
    "    \n",
    "    if len(x.shape)==1: \n",
    "        x = X_.flatten()\n",
    "    else:\n",
    "        x = X_\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "print (\"=== For autograder ===\")\n",
    "print (softmax(np.array([[1001,1002],[3,4]])))\n",
    "print (softmax(np.array([[-1001,-1002]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sigmoid \n",
    "\n",
    "The sigmoid function is defined as $sigmoid(\\mathbf{x})_i = \\frac{1}{1+{e^{-x_i}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove math import depending on piazza answer\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    #The largest value representable by a numpy float64 is 1.7976931348623157e+308, \n",
    "    #whose logarithm is about 709.782712893384\n",
    "    #So, we clip all values<-709.782712893384 to -709.782712893384\n",
    "    \n",
    "     #The least value(>0) representable by a numpy float64 is 2.2204460492503131e-16, \n",
    "    #whose logarithm is about -36.04365338911715\n",
    "    #So, we clip all values>36.04365338911715 to 36.04365338911715\n",
    "    \n",
    "    \n",
    "    #largest_float64=math.log(np.finfo(np.float64).max)\n",
    "    #least_value_greater_zero=math.log(np.finfo(np.float64).eps)\n",
    "    \n",
    "    largest_float64=709.782712893384\n",
    "    least_value_greater_zero=-36.04365338911715\n",
    "    y=np.clip(x,-1*largest_float64,-1*least_value_greater_zero)\n",
    "    x=1/(1+np.exp(-1*y))\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    y=np.multiply(f,(1-f))\n",
    "\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "f = sigmoid(x)\n",
    "g = sigmoid_grad(f)\n",
    "print (\"=== For autograder ===\")\n",
    "print (f)\n",
    "print (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Checking\n",
    "\n",
    "[Gradient checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) is an important technique for debugging the gradient computation. Logistic regression is a relatively simple algorithm where it is straightforward to derive and implement its cost function and gradient computation. For more complex models, the gradient computaitn can be notoriously difficulty to debug and get right. Sometimes a subtly buggy implementation will manage to learn something that can look surprisingly reasonable, while performing less well than a correct implementation. Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        ### YOUR CODE HERE:\n",
    "\n",
    "        e=x.astype('float64')\n",
    "        e[ix]+=h\n",
    "        random.setstate(rndstate)\n",
    "        f_plus_epsilon,grad_plus=f(e)\n",
    "        e[ix]-=2.0*h\n",
    "        random.setstate(rndstate)\n",
    "        f_minus_epsilon,grad_minus=f(e)\n",
    "        numgrad=(f_plus_epsilon-f_minus_epsilon)/(2*h)\n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print (\"Gradient check failed.\")\n",
    "            print (\"First gradient error found at index %s\" % str(ix))\n",
    "            print (\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print (\"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print (\"=== For autograder ===\")\n",
    "gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the functions you just implemented, fill in the following functions to implement a neural network with one sigmoid hidden layer. You may skip this 1.4 section, and implement the word2vec below directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for testing the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "\n",
    "for i in range(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"forward_backward_prop\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1])) # (10,5)\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1])) # (1, 5)\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))  # (5, 10)\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2])) # (1, 10)\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation (calculate the cost as cross entropy error)\n",
    "    x=data\n",
    "    y=labels\n",
    "\n",
    "    z1=np.dot(x,W1)+b1\n",
    "    h=sigmoid(z1)\n",
    "    z2=np.dot(h,W2)+b2\n",
    "    y_=softmax(z2)\n",
    "    \n",
    "    # Calculating the loss\n",
    "    cost = np.sum(-np.multiply(y,np.log(y_)))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    delta3=y_-y\n",
    "    delta2=np.multiply(np.dot(delta3,W2.T),sigmoid_grad(h))\n",
    "    \n",
    "    gradW2=np.dot(h.T,delta3)\n",
    "    gradb2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    gradW1=np.dot(x.T,delta2)\n",
    "    gradb1 = np.sum(delta2, axis=0, keepdims=True)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "print (\"=== For autograder ===\")\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2vec [Bonus 15 Points]\n",
    "\n",
    "In this part you will implement the `word2vec` models and train your own word vectors with stochastic gradient descent (SGD). You may find the following readings helpful:\n",
    "\n",
    "- [Lecture Note on Skip-gram, CBOW and Negative Sampling](http://cs224d.stanford.edu/lecture_notes/notes1.pdf) by Francois Chaubard, Rohit Mundra, Richard Socher\n",
    "- [Semantics with Dense Vectors](https://web.stanford.edu/~jurafsky/slp3/16.pdf) (Speech and Language Processing, Chapter 16) by Dan Jurafsky and James H. Martin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x_norm=np.linalg.norm(x,ord = 2,axis=1,keepdims=True)\n",
    "    x = x/x_norm\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test this function\n",
    "print (\"=== For autograder ===\")\n",
    "print (normalizeRows(np.array([[3.0,4.0],[1, 2]])))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens     \n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                                                                             \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    u_c=outputVectors[target]\n",
    "    y_=softmax(outputVectors.dot(predicted))\n",
    "    cost=-1.0*np.sum(np.log(y_[target]))\n",
    "    gradPred=np.sum(y_.reshape(-1,1)*outputVectors, axis=0)-u_c\n",
    "    grad=y_.reshape(-1,1)*np.tile(predicted,(outputVectors.shape[0],1))\n",
    "    grad[target] -= predicted\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    indices=[None]*K\n",
    "    for k in range(K):\n",
    "        randomIndex=dataset.sampleTokenIdx()\n",
    "        while randomIndex==target:\n",
    "            randomIndex=dataset.sampleTokenIdx()\n",
    "        indices[k]=randomIndex\n",
    "    \n",
    "    NegSamples=outputVectors[indices].reshape(len(indices),-1)\n",
    "    u_o=outputVectors[target]\n",
    "    \n",
    "    sigmoid1=sigmoid(predicted.dot(u_o))\n",
    "    sigmoid2=sigmoid(-1.0*NegSamples.dot(predicted))\n",
    "    \n",
    "    term1=-1.0*np.log(sigmoid1)\n",
    "    term2=-1.0*np.sum(np.log(sigmoid2))\n",
    "    cost=term1+term2\n",
    "    \n",
    "    term1=np.sum(sigmoid1)-1.0\n",
    "    term2=-1.0*np.sum((sigmoid2.reshape(-1,1)-1.0)*NegSamples,axis=0)\n",
    "    gradPred=term1*u_o+term2\n",
    "    \n",
    "    value=-1.0*(sigmoid2.reshape(-1,1)-1.0)\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    grad[target] +=term1*(predicted)\n",
    "    np.add.at(grad, indices, value*np.tile(predicted,(K,1)))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    index_v_c=tokens[currentWord]\n",
    "    predicted=inputVectors[index_v_c]\n",
    "    \n",
    "    for i in contextWords:\n",
    "        target=tokens[i] \n",
    "        cost_,gradPred,grad=word2vecCostAndGradient(predicted,target,outputVectors,dataset)\n",
    "        gradIn[index_v_c]+=gradPred\n",
    "        gradOut+=grad\n",
    "        cost+=cost_\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the continuous bag-of-words model in this function.            \n",
    "    # Input/Output specifications: same as the skip-gram model        \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    context_index=[tokens[contextword] for contextword in contextWords]\n",
    "    predicted = np.zeros(inputVectors.shape[1])\n",
    "    for idx in context_index:\n",
    "        predicted += inputVectors[idx]\n",
    "        \n",
    "    target=tokens[currentWord]\n",
    "    \n",
    "    cost,gradPred,grad=word2vecCostAndGradient(predicted,target,outputVectors,dataset)\n",
    "    \n",
    "    for i in context_index:\n",
    "        gradIn[i]+=gradPred\n",
    "        \n",
    "    gradOut+=grad\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(14.093692760899629, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-3.86802836, -1.12713967, -1.52668625],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.11265089,  0.05169237,  0.39321163],\n",
      "       [-0.22716495,  0.10423969,  0.79292674],\n",
      "       [-0.79674766,  0.36560539,  2.78107395],\n",
      "       [-0.31602611,  0.14501561,  1.10309954],\n",
      "       [-0.80620296,  0.36994417,  2.81407799]]))\n",
      "(0.79899580109066504, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.8955932035991392, array([[-2.98873309, -3.38440688, -2.62676289],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-0.77702167, -0.21080061,  0.51922198],\n",
      "       [-2.58955401, -0.7025281 ,  1.73039366],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check!\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:int(N/2),:]\n",
    "    outputVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:int(N/2), :] += gin / batchsize / denom\n",
    "        grad[int(N/2):, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print (\"==== Gradient check for skip-gram ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "print (\"\\n==== Gradient check for CBOW      ====\")\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "print (\"\\n=== Results ===\")\n",
    "print (skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "print (skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n",
    "print (cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset))\n",
    "print (cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n"
     ]
    }
   ],
   "source": [
    "# Now, implement SGD\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import os.path as op\n",
    "import pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"./saved_params/saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"./saved_params/saved_params_%d.npy\" % st, \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"./saved_params/saved_params_%d.npy\" % iter, \"wb\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    # Implement the stochastic gradient descent method in this        \n",
    "    # function.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - f: the function to optimize, it should take a single        \n",
    "    #     argument and yield two outputs, a cost and the gradient  \n",
    "    #     with respect to the arguments                            \n",
    "    # - x0: the initial point to start SGD from                     \n",
    "    # - step: the step size for SGD                                 \n",
    "    # - iterations: total iterations to run SGD for                 \n",
    "    # - postprocessing: postprocessing function for the parameters  \n",
    "    #     if necessary. In the case of word2vec we will need to    \n",
    "    #     normalize the word vectors to have unit length.          \n",
    "    # - PRINT_EVERY: specifies every how many iterations to output  \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - x: the parameter value after SGD finishes  \n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        cost,gradient=f(x)\n",
    "        x-=step*gradient\n",
    "\n",
    "\n",
    "        \n",
    "        ### END YOUR CODE\n",
    " \n",
    "       ### You might want to print the progress every few iterations.\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print (\"iter %d: %f\" % (iter, expcost))\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            print (x)\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x\n",
    "\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print (\"Running sanity checks...\")\n",
    "t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "print (\"test 1 result:\", t1)\n",
    "assert abs(t1) <= 1e-6\n",
    "\n",
    "t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "print (\"test 2 result:\", t2)\n",
    "assert abs(t2) <= 1e-6\n",
    "    \n",
    "t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "print (\"test 3 result:\", t3)\n",
    "assert abs(t3) <= 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show time! Now we are going to load some real data and train word vectors with everything you just implemented!**\n",
    "\n",
    "We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load some data and initialize word vectors\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "\n",
      "[[-0.51931623 -0.28509669 -0.52266407  0.4908477   0.88114791  0.09642419\n",
      "   0.0446164   0.64596639 -0.84448083 -0.0736909 ]\n",
      " [-0.42071929 -0.24434525 -0.48779761  0.49601216  0.80033961  0.08115494\n",
      "   0.00215316  0.57562664 -0.8598679  -0.16520325]\n",
      " [-0.31498762 -0.13657819 -0.40661577  0.42110174  0.70694608  0.18019722\n",
      "   0.04327356  0.44304365 -0.5555805  -0.09306423]\n",
      " [-0.43531545 -0.27924067 -0.44041543  0.37313175  0.64919439  0.11256128\n",
      "   0.01104744  0.53314682 -0.58239265 -0.08479977]\n",
      " [-0.15619474 -0.04414976 -0.1810108   0.17316944  0.19992665  0.0719525\n",
      "  -0.03034347  0.12005638 -0.10239862 -0.05695865]\n",
      " [-0.39058926 -0.20398315 -0.38580608  0.46892391  0.60927928  0.08565322\n",
      "  -0.0164034   0.43434016 -0.55720563 -0.10417561]\n",
      " [-0.40728585 -0.22558091 -0.55979721  0.49655131  0.79217106  0.08801835\n",
      "  -0.03554238  0.59981041 -0.67141297 -0.07427388]]\n"
     ]
    }
   ],
   "source": [
    "# Train word vectors (this could take a while!)\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / dimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient), \n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "\n",
    "# sanity check: cost at convergence should be around or below 10\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "\n",
    "\n",
    "print (\"=== For autograder ===\")\n",
    "print ()\n",
    "checkWords = [\"the\", \"a\", \"an\", \"movie\", \"ordinary\", \"but\", \"and\"]\n",
    "checkIdx = [tokens[word] for word in checkWords]\n",
    "checkVecs = wordVectors[checkIdx, :]\n",
    "print (checkVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1567778803419233, 0.13596363506173773)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHaCAYAAABB+z4aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a553518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Visualize the word vectors you trained\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \"good\", \"great\", \"cool\", \"brilliant\", \n",
    "                  \"wonderful\", \"well\", \"amazing\",\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \n",
    "                  \"dumb\", \"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], bbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
